tiny_lm:
  arch: gru
  vocab_size: 512
  emb_dim: 32
  hidden_dim: 64
  num_layers: 1
  max_seq_len: 64

train:
  lr: 1e-3
  grad_clip: 1.0
  sanity_ppo_steps: 2

eval:
  max_new_tokens: 16

ppo:
  kl_beta: 0.1
