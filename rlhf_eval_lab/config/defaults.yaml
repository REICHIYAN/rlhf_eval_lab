tiny_lm:
  arch: gru
  vocab_size: 512
  emb_dim: 32
  hidden_dim: 64
  num_layers: 1
  max_seq_len: 64

train:
  grad_clip: 1.0
  sanity_adv_scale: 10.0
  sanity_ppo_steps: 2
  lr: 1e-3
  hf_sft_steps: 0
  hf_max_seq_len: 256
  hf_ppo_steps: 0
  ppo_clip: 0.2
  ppo_lr: 1e-6

eval:
  max_new_tokens: 16

ppo:
  kl_beta: 0.1

hf:
  model_name: gpt2
  temperature: 1.0
